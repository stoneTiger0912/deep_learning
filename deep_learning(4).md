# 4 신경망 학습
- - - -
**4.1** 신경망 특징
데이터를 보고 학습
가중치 매개변수 값을 자동으로 결정

4.1.1 데이터 주도 학습
	* 이미지에서 특징을 추출하고 패턴을 학습하는 방법
	* 기계가 스스로 학습

4.1.2 훈련 데이터와 시험 데이터
	* 훈련 데이터만 사용하여 학습후 최적의 매개변수를 찾음
	* 시험 데이터로 훈련한 모델의 실력을 평가
훈련 데이터와 시험 데이터를 나누는 이유
	* 범용능력(아직 보지못한 데이터)를 평가하기 위해
	* 한 데이터셋만 지나치게 최적화된 상태를 오버피팅

**4.2** 손실 함수
	* 신경망에서는 현재의 상태를 '하나의 지표'로 표현
	* 학습에서 사용되는 지표를 손실 함수
	* 오차 제곱합과 교차 엔트로피의 오차를 사용

4.2.1 오차제곱합
![](4%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5/bear_sketch@2x.png)
	* yk: 신경망이 추정한 값
	* tk: 정답 레이블
	* k: 데이터의 차원수
```python
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```
	* 소프트 맥스 함수로 되어있어서 확률로 해석
	* 숫자가 2일 때의 원소 값이 1이므로 정답이 2
	* 원-핫 인코딩
	
오차 제곱합 구현
```python
def sum_squares_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```
	* y와 t는 넘파이 배열
	* 오차가 작을수록 답에 근접

4.2.2 교차 엔트로피 오차
	![](4%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5/bear_sketch@2x.png)
	* log는 자연로그 ln 
	
교차 엔트로피 오차구현
```python
def cross_entropy_error(y, t):
    delta = 1e-7
	  return -np.sum(t * np.log(y + delta))
```
	* delta를 더하는 이유는 0이 입력될수 있도록 함
	* 오차가 작을수록 답에 근접

4.2.3 미니배치 학습
	* 훈련데이터를 사용하면 모든 훈련데이터의 손실함수값을 구해야함

![](4%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5/bear_sketch@2x.png)

	* 데이터의 개수가 N개일때 tnk는 t의 n번째 데이터의 k값
	* N으로 마지막에 나눠 정규화 하여 평균 손실 함수를 구함
	* 데이터가 많을경우 일부만 무작위로 골라 학습을 수행

```python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
# 지정한 범위의 수 중에 무작위로 원하는 개수만 구함
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```
